{"cells":[{"cell_type":"markdown","metadata":{"id":"2tee0ZY2P4zL"},"source":["# Import Statements"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":3354,"status":"ok","timestamp":1707645181027,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"V3rIbpnjP4zO"},"outputs":[],"source":["import pandas as pd # Imported to enable the use of datastructures like dataframe\n","import matplotlib.pyplot as plt # Imported to visusalise data\n","import seaborn as sns # Imported to visualise data\n","import numpy as np # Imported for calculations\n","import json\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","import emoji\n","from sklearn.svm import SVC\n","from sklearn.feature_extraction.text import TfidfVectorizer # Imported to convert raw documents into a matrix of tf idf features\n","from sklearn.linear_model import LogisticRegression # Imported to enable the use of logistic regression to classify text\n","from sklearn.model_selection import train_test_split # Imported to enable the user to split the data into train, test samples\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score # for report generation"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15080,"status":"ok","timestamp":1707645196104,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"wTBlsHIKQUvI","outputId":"48620db6-880d-4b89-f6c2-43c5c11e0229"},"outputs":[{"name":"stdout","output_type":"stream","text":["Not able to open google drive, opening files locally\n"]}],"source":["local=False\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    print(\"Able to open google drive!\")\n","except:\n","    print(\"Not able to open google drive, opening files locally\")\n","    local=True\n","else:\n","  print(\"Nothing went wrong\") "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["if(not local):\n","    path_train = '/content/drive/MyDrive/Datasets/malayalam_train.tsv'\n","    path_val = '/content/drive/MyDrive/Datasets/malayalam_dev.tsv'\n","    path_test = '/content/drive/MyDrive/Datasets/malayalam_test.tsv'\n","else:\n","    path_train='/Users/sachin/Library/CloudStorage/GoogleDrive-heysachins@gmail.com/My Drive/Datasets/malayalam_train.tsv'\n","    path_test='/Users/sachin/Library/CloudStorage/GoogleDrive-heysachins@gmail.com/My Drive/Datasets/malayalam_test.tsv'\n","    path_val='/Users/sachin/Library/CloudStorage/GoogleDrive-heysachins@gmail.com/My Drive/Datasets/malayalam_dev.tsv'"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/sachin/Library/CloudStorage/GoogleDrive-heysachins@gmail.com/My Drive/Datasets/malayalam_test.tsv\n"]}],"source":["print(path_test)"]},{"cell_type":"markdown","metadata":{"id":"YFdlSs2cP4zP"},"source":["# Importing the dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":975,"status":"ok","timestamp":1707645198586,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"7lIeBE_dP4zP","outputId":"ab7343bd-dde8-4a80-be90-bf5b2a0614f6"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                   text         category\n","0                hoo mammokka police vesham aaha anthas        Positive \n","1        Oru rekshayum illa...kidilam kannu nananjupoyi        Positive \n","2                             Ikka     waiting.........        Positive \n","3                Raju Ettante Oro Shorttum Ijathi ppwli        Positive \n","4      Ettan fansil netti poya aarenkilum undo?    #...        Positive \n","...                                                 ...              ...\n","4846   Madhuraraja trailer Kand ivide vannanvar likkeee   unknown_state \n","4847   Njn pru lalettan fan ahn..  eee trailer mass ...        Positive \n","4848   Valiya pratheesha illa nalla entertainment  a...  Mixed_feelings \n","4849   Dislike adikkunna kazhuthakalude mukhath adik...        Negative \n","4850   Adipoli..... Pakshe oru sankadam ithinte thir...  Mixed_feelings \n","\n","[4851 rows x 2 columns]\n","                                                  text        category\n","0                               speechless ü§ê.   ikkaaa  not-malayalam \n","1     Raja sollunathu mattuthaam seyyvaa seyyunnath...  not-malayalam \n","2         Im Prithiviraj fan from tamilnadu... Love it  not-malayalam \n","3                   mohanlal sir - look ..... kiddo...       Positive \n","4            Kandathil vech mungiya pdam  Rating 1.1/5       Negative \n","..                                                 ...             ...\n","535   Mammuka happy birthday stay blessed live long...       Positive \n","536      Like from superstar thalaivar rajini sir fans  not-malayalam \n","537   Mamangam keralam kanda ettavum valliya hit akkum       Positive \n","538   FDFS.. trailer kandu ithu vareyum njanithu pa...       Positive \n","539   1:38 enikkathu inganaaa kelkkunne.. mammoookk...       Positive \n","\n","[540 rows x 2 columns]\n","               id                                               text  \\\n","0        ml_sen_1            Bollywood film Newton inte remake aano?   \n","1        ml_sen_2  endukond viewrs koodunnilla ?? ippozhum 2.8m a...   \n","2        ml_sen_3  Mara paazhu mega mairananil ninnum ethil koodu...   \n","3        ml_sen_4                       Video nay cang xem cang thit   \n","4        ml_sen_5  Sunny chechiye kaanan vannathu njan maathram aano   \n","...           ...                                                ...   \n","1343  ml_sen_1344                          143k views and 145k likes   \n","1344  ml_sen_1345               Nasik doll tiltle bgm aane highlight   \n","1345  ml_sen_1346  Mohanlalile ishtapetta vekthiyayirunnu njan pa...   \n","1346  ml_sen_1347            ivaruku ivlo age nu sonna evan nambuvan   \n","1347  ml_sen_1348  Kanditt Amala Paul Aadai tamil mattoru version...   \n","\n","           category  \n","0     unknown_state  \n","1     unknown_state  \n","2          Negative  \n","3     not-malayalam  \n","4     unknown_state  \n","...             ...  \n","1343  unknown_state  \n","1344  unknown_state  \n","1345       Negative  \n","1346  not-malayalam  \n","1347  unknown_state  \n","\n","[1348 rows x 3 columns]\n"]}],"source":["df_train = pd.read_csv(path_train, sep='\\t')\n","print(df_train)\n","\n","df_val = pd.read_csv(path_val, sep='\\t')\n","print(df_val)\n","\n","df_test = pd.read_csv(path_test, sep='\\t')\n","print(df_test)"]},{"cell_type":"markdown","metadata":{"id":"8XvZE3KyP4zQ"},"source":["# Exploring the dataset"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1707645198586,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"D6ff2De2P4zQ","outputId":"338e922c-e8b6-4b56-e0cf-7f64551be39b"},"outputs":[{"name":"stdout","output_type":"stream","text":["(4851, 2)\n","(1348, 2)\n","(540, 2)\n","Total =  6739\n"]}],"source":["print(df_train.shape)\n","print(df_test.shape)\n","print(df_val.shape)\n","\n","total=df_val.shape[0]+df_test.shape[0]+df_train.shape[0]\n","print(\"Total = \",total)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["already dropped column\n","(6739, 2)\n"]}],"source":["try:\n","    df_test = df_test.drop('id', axis=1)\n","except:\n","    print(\"already dropped column\")\n","\n","df_dataset = pd.concat([df_train, df_val, df_test], ignore_index=True)\n","print(df_dataset.shape)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive          2811\n","Neutral           1903\n","Not-Malayalam      884\n","Negative           738\n","Mixed_feelings     403\n","Name: category, dtype: int64\n"]}],"source":["# Renaming (df_train) the classes for convenience\n","\n","# Removing any leading/trailing spaces\n","df_dataset['category'] = df_dataset['category'].str.strip()\n","\n","# Replacing 'unknown_state' with 'Irrelevant'\n","df_dataset['category'] = df_dataset['category'].replace({'unknown_state': 'Neutral'})\n","df_dataset['category'] = df_dataset['category'].replace({'not-malayalam': 'Not-Malayalam'})\n","\n","# Viewing the number of items in each class after replacement\n","print(df_dataset['category'].value_counts())  # Used to view the number of items in each class.\n","\n","## There is a significant imbalance in the classes in this dataset"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1707645198586,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"AGaa3zvJP4zR","outputId":"7c35dbcc-d545-4d1e-c1b4-965655a8a7c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive          2022\n","Neutral           1344\n","Not-Malayalam      647\n","Negative           549\n","Mixed_feelings     289\n","Name: category, dtype: int64\n"]}],"source":["# Renaming (df_train) the classes for convenience\n","\n","# Removing any leading/trailing spaces\n","df_train['category'] = df_train['category'].str.strip()\n","\n","# Replacing 'unknown_state' with 'Irrelevant'\n","df_train['category'] = df_train['category'].replace({'unknown_state': 'Neutral'})\n","df_train['category'] = df_train['category'].replace({'not-malayalam': 'Not-Malayalam'})\n","\n","# Viewing the number of items in each class after replacement\n","print(df_train['category'].value_counts())  # Used to view the number of items in each class.\n","\n","## There is a significant imbalance in the classes in this dataset"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1707645198863,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"Pzj0d6KZuBN3","outputId":"928abf87-0a82-4da0-c91d-f499a23b2866"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive          565\n","Neutral           398\n","Not-Malayalam     177\n","Negative          138\n","Mixed_feelings     70\n","Name: category, dtype: int64\n"]}],"source":["# Renaming (df_test) the classes for convenience\n","\n","# Removing any leading/trailing spaces\n","df_test['category'] = df_test['category'].str.strip()\n","\n","# Replacing 'unknown_state' with 'Irrelevant'\n","df_test['category'] = df_test['category'].replace({'unknown_state': 'Neutral'})\n","df_test['category'] = df_test['category'].replace({'not-malayalam': 'Not-Malayalam'})\n","\n","# Viewing the number of items in each class after replacement\n","print(df_test['category'].value_counts())  # Used to view the number of items in each class.\n","\n","## There is a significant imbalance in the classes in this dataset"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707645198863,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"7y8ZXuIPuG6C","outputId":"6445267b-77fa-467e-f4b7-24a500cfdec5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Positive          224\n","Neutral           161\n","Not-Malayalam      60\n","Negative           51\n","Mixed_feelings     44\n","Name: category, dtype: int64\n"]}],"source":["# Renaming (df_val) the classes for convenience\n","\n","# Removing any leading/trailing spaces\n","df_val['category'] = df_val['category'].str.strip()\n","\n","# Replacing 'unknown_state' with 'Irrelevant'\n","df_val['category'] = df_val['category'].replace({'unknown_state': 'Neutral'})\n","df_val['category'] = df_val['category'].replace({'not-malayalam': 'Not-Malayalam'})\n","\n","# Viewing the number of items in each class after replacement\n","print(df_val['category'].value_counts())  # Used to view the number of items in each class.\n","\n","## There is a significant imbalance in the classes in this dataset"]},{"cell_type":"markdown","metadata":{},"source":["# Creating dataframes for each dataset (WholeDataset, Test, Train, Validation)"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(2811, 2)\n","(738, 2)\n","(403, 2)\n","(1903, 2)\n","(884, 2)\n"]}],"source":["# Creating dataframes for (df_train) all categories for later use\n","\n","df_dataset_positive_words = df_dataset[df_dataset['category']=='Positive']\n","df_dataset_negative_words = df_dataset[df_dataset['category']=='Negative']\n","df_dataset_mixed_feeling_words = df_dataset[df_dataset['category']=='Mixed_feelings']\n","df_dataset_neutral_words = df_dataset[df_dataset['category']=='Neutral']\n","df_dataset_not_malayalam_words = df_dataset[df_dataset['category']=='Not-Malayalam']\n","\n","print(df_dataset_positive_words.shape)\n","print(df_dataset_negative_words.shape)\n","print(df_dataset_mixed_feeling_words.shape)\n","print(df_dataset_neutral_words.shape)\n","print(df_dataset_not_malayalam_words.shape)"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1707645199111,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"ETd-wLNngMjQ","outputId":"7c3e4ed8-aa94-4a8c-ffdc-d08b4529e366"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2022, 2)\n","(549, 2)\n","(289, 2)\n","(1344, 2)\n","(647, 2)\n"]}],"source":["# Creating dataframes for (df_train) all categories for later use\n","\n","df_train_positive_words = df_train[df_train['category']=='Positive']\n","df_train_negative_words = df_train[df_train['category']=='Negative']\n","df_train_mixed_feeling_words = df_train[df_train['category']=='Mixed_feelings']\n","df_train_neutral_words = df_train[df_train['category']=='Neutral']\n","df_train_not_malayalam_words = df_train[df_train['category']=='Not-Malayalam']\n","\n","print(df_train_positive_words.shape)\n","print(df_train_negative_words.shape)\n","print(df_train_mixed_feeling_words.shape)\n","print(df_train_neutral_words.shape)\n","print(df_train_not_malayalam_words.shape)"]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1707645199384,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"4hVnafhFuyLq","outputId":"edf2abc4-51b1-469c-9221-61aba1388d41"},"outputs":[{"name":"stdout","output_type":"stream","text":["(565, 2)\n","(138, 2)\n","(70, 2)\n","(398, 2)\n","(177, 2)\n"]}],"source":["# Creating dataframes for (df_test) all categories for later use\n","\n","df_test_positive_words = df_test[df_test['category']=='Positive']\n","df_test_negative_words = df_test[df_test['category']=='Negative']\n","df_test_mixed_feeling_words = df_test[df_test['category']=='Mixed_feelings']\n","df_test_neutral_words = df_test[df_test['category']=='Neutral']\n","df_test_not_malayalam_words = df_test[df_test['category']=='Not-Malayalam']\n","\n","print(df_test_positive_words.shape)\n","print(df_test_negative_words.shape)\n","print(df_test_mixed_feeling_words.shape)\n","print(df_test_neutral_words.shape)\n","print(df_test_not_malayalam_words.shape)"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1707645199631,"user":{"displayName":"Sachin Sasidharan Nair","userId":"10920055553907819489"},"user_tz":0},"id":"SiHbUK1qvFXW","outputId":"b825b841-8c8b-4ff4-806a-265a55ca5393"},"outputs":[{"name":"stdout","output_type":"stream","text":["(224, 2)\n","(51, 2)\n","(44, 2)\n","(161, 2)\n","(60, 2)\n"]}],"source":["# Creating dataframes for (df_val) all categories for later use\n","\n","df_val_positive_words = df_val[df_val['category']=='Positive']\n","df_val_negative_words = df_val[df_val['category']=='Negative']\n","df_val_mixed_feeling_words = df_val[df_val['category']=='Mixed_feelings']\n","df_val_neutral_words = df_val[df_val['category']=='Neutral']\n","df_val_not_malayalam_words = df_val[df_val['category']=='Not-Malayalam']\n","\n","print(df_val_positive_words.shape)\n","print(df_val_negative_words.shape)\n","print(df_val_mixed_feeling_words.shape)\n","print(df_val_neutral_words.shape)\n","print(df_val_not_malayalam_words.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Descrpancies Found"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Database is imbalanced, nothing has been done to rectify this.\n","\n","value_counts = df_dataset['category'].value_counts()\n","print(value_counts)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["236 comments have more than 15 words\n","346 comments have less than 5 words\n","309 sentences have emojis in them\n","891 sentences were not preprocessed\n"]}],"source":["# Pre processing is not done as mentioned in the paper.\n","\n","# We preprocessed the comments by removing the emoji‚Äôs,\n","# and sentence length longer than 15 or less than 5 words since\n","# sentence more than 15 words will be difficult for annotators.\n","# After cleaning, we got 6,738 sentences for Malayalam-English\n","# code-mixed post comments.\n","\n","# Function to count words in a sentence\n","def count_words(sentence):\n","    return len(sentence.split())\n","\n","# Apply the function and filter the DataFrame\n","df_wordsGreater15 = df_dataset[df_dataset['text'].apply(lambda x: count_words(x) > 15)]\n","df_wordsLess5 = df_dataset[df_dataset['text'].apply(lambda x: count_words(x) < 5)]\n","\n","\n","# Function to check if a sentence contains any emoji\n","def contains_emoji(sentence):\n","    for character in sentence:\n","        if emoji.is_emoji(character):\n","            return True\n","    return False\n","\n","# Apply the function and count sentences with emojis\n","df_emojis = df_dataset[df_dataset['text'].apply(contains_emoji)]\n","\n","print(str(df_wordsGreater15.shape[0]) + \" comments have more than 15 words\") #236 sentences have more than 15 words\n","print(str(df_wordsLess5.shape[0]) + \" comments have less than 5 words\") #346 sentences have less than 5 words\n","print(str(df_emojis.shape[0]) + \" sentences have emojis in them\") #309 sentences have emojis in them\n","print(str(df_wordsGreater15.shape[0]+df_wordsLess5.shape[0]+df_emojis.shape[0]) + \" sentences were not preprocessed\")"]},{"cell_type":"markdown","metadata":{},"source":["# Computing the statistics using Tokenize from NLTK"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of tokens: 61022\n","Total number of sentences: 7787\n","Total number of words: 61022\n","Vocabulary Size: 19389\n","Average sentence length: 8.265447395756048\n","Average sentence per comment: 1.1555126873423356\n"]}],"source":["all_words = set()\n","def analyze_text(text):\n","    # Tokenizes the text into sentences\n","    sentences = sent_tokenize(text)\n","    sentence_count = len(sentences)\n","\n","    # Tokenizes the text into words\n","    words = word_tokenize(text)\n","    word_count = len(words)\n","\n","    # Computes the vocabulary size\n","    # vocabulary_size = len(set(words))\n","    for i in words:\n","        all_words.add(i)\n","\n","    # Calculates average sentence length in terms of words\n","    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n","\n","    return sentence_count, word_count, avg_sentence_length\n","\n","# Apply the analysis function to each text entry in the DataFrame\n","analysis_results = df_dataset['text'].apply(lambda x: analyze_text(x))\n","\n","# Unpack the results into separate columns\n","df_dataset['sentence_count'], df_dataset['word_count'],  df_dataset['avg_sentence_length'] = zip(*analysis_results)\n","\n","# To find the total number of tokens, you can sum up the word counts\n","total_tokens = df_dataset['word_count'].sum()\n","\n","# total sentence count\n","total_sentence_count = df_dataset['sentence_count'].sum()\n","\n","#total word_count\n","total_word_count = df_dataset['word_count'].sum()\n","\n","#total vocabulary\n","# total_vocabulary_count = df_dataset['vocabulary_size'].sum()\n","\n","#avg sentence_length\n","avg_sentence_length = df_dataset['avg_sentence_length'].mean()\n","\n","# avg sentence per post\n","avg_sentence_per_comment = df_dataset['sentence_count'].mean()\n","\n","# Display the overall analysis results and the updated DataFrame\n","print(f\"Total number of tokens: {total_tokens}\")\n","print(f\"Total number of sentences: {total_sentence_count}\")\n","print(f\"Total number of words: {total_word_count}\")\n","print(f\"Vocabulary Size: {len(all_words)}\")\n","print(f\"Average sentence length: {avg_sentence_length}\")\n","print(f\"Average sentence per comment: {avg_sentence_per_comment}\")\n","\n","# print(df_dataset.head())  # Adjust according to how you wish to view the results\n"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"data":{"text/plain":["category\n","Positive          2811\n","Neutral           1903\n","Not-Malayalam      884\n","Negative           738\n","Mixed_feelings     403\n","dtype: int64"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["df_dataset.value_counts('category')"]},{"cell_type":"markdown","metadata":{},"source":["# Computing the statistics using SPACY"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting spacy\n","  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/ca/f3/609bb7512cad1f02af13daa23aa433b931da34c502211f29fd47dceff624/spacy-3.7.2-cp311-cp311-macosx_11_0_arm64.whl.metadata\n","  Downloading spacy-3.7.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (25 kB)\n","Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n","  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n","Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n","  Obtaining dependency information for spacy-loggers<2.0.0,>=1.0.0 from https://files.pythonhosted.org/packages/33/78/d1a1a026ef3af911159398c939b1509d5c36fe524c7b644f34a5146c4e16/spacy_loggers-1.0.5-py3-none-any.whl.metadata\n","  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n","Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n","  Obtaining dependency information for murmurhash<1.1.0,>=0.28.0 from https://files.pythonhosted.org/packages/7a/05/4a3b5c3043c6d84c00bf0f574d326660702b1c10174fe6b44cef3c3dff08/murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata\n","  Downloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n","Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n","  Obtaining dependency information for cymem<2.1.0,>=2.0.2 from https://files.pythonhosted.org/packages/d7/f6/67babf1439cdd6d46e4e805616bee84981305c80e562320c293712f54034/cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n","  Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n","Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n","  Obtaining dependency information for preshed<3.1.0,>=3.0.2 from https://files.pythonhosted.org/packages/a8/b3/1a73ba16bab53043fd19dd0a7838ae05c705dccb329404dd4ad5925767f1/preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata\n","  Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n","Collecting thinc<8.3.0,>=8.1.8 (from spacy)\n","  Obtaining dependency information for thinc<8.3.0,>=8.1.8 from https://files.pythonhosted.org/packages/05/48/2cf60744d60d07d789ce5cf6230fe2140612bc3f8ae70a89bc980ea27a28/thinc-8.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n","  Downloading thinc-8.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n","Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n","  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n","  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n","Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n","  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/1b/d7/0800af1a75008b3a6a6a24f3efd165f2d2208076e9b8a4b11b66f16217f3/srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata\n","  Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n","Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n","  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/9e/96/d32b941a501ab566a16358d68b6eb4e4acc373fab3c3c4d7d9e649f7b4bb/catalogue-2.0.10-py3-none-any.whl.metadata\n","  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n","Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n","  Obtaining dependency information for weasel<0.4.0,>=0.1.0 from https://files.pythonhosted.org/packages/d5/e5/b63b8e255d89ba4155972990d42523251d4d1368c4906c646597f63870e2/weasel-0.3.4-py3-none-any.whl.metadata\n","  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n","Collecting typer<0.10.0,>=0.3.0 (from spacy)\n","  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy) (5.2.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy) (4.65.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy) (1.10.8)\n","Requirement already satisfied: jinja2 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy) (3.1.2)\n","Requirement already satisfied: setuptools in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy) (68.0.0)\n","Requirement already satisfied: packaging>=20.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy) (23.0)\n","Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy) (1.23.5)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n","Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.1.8->spacy)\n","  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/a8/73/0a9d4e7f6e78ef270e3a4532b17e060a02087590cf615ba9943fd1a283e9/blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata\n","  Downloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n","Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.1.8->spacy)\n","  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/39/78/f9d18da7b979a2e6007bfcea2f3c8cc02ed210538ae1ce7e69092aed7b18/confection-0.1.4-py3-none-any.whl.metadata\n","  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n","Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n","  Obtaining dependency information for cloudpathlib<0.17.0,>=0.7.0 from https://files.pythonhosted.org/packages/0f/6e/45b57a7d4573d85d0b0a39d99673dc1f5eea9d92a1a4603b35e968fbf89a/cloudpathlib-0.16.0-py3-none-any.whl.metadata\n","  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from jinja2->spacy) (2.1.1)\n","Downloading spacy-3.7.2-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n","Downloading cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n","Downloading preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m128.8/128.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n","Downloading srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m488.4/488.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading thinc-8.2.3-cp311-cp311-macosx_11_0_arm64.whl (781 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m781.1/781.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n","Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading confection-0.1.4-py3-none-any.whl (35 kB)\n","Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, murmurhash, langcodes, cloudpathlib, catalogue, blis, srsly, preshed, confection, weasel, thinc, spacy\n","Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 spacy-3.7.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.0 wasabi-1.1.2 weasel-0.3.4\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.8)\n","Requirement already satisfied: jinja2 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n","Requirement already satisfied: setuptools in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n","Requirement already satisfied: packaging>=20.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n","Requirement already satisfied: numpy>=1.19.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.23.5)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n","Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n","Installing collected packages: en-core-web-sm\n","Successfully installed en-core-web-sm-3.7.1\n","\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}],"source":["!pip install spacy\n","!python -m spacy download en_core_web_sm\n"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of tokens: 63104.0\n","Total number of sentences: 9051.0\n","Total number of words: 63104.0\n","Vocabulary Size: 59758.0\n","Average sentence length: 7.813441658010585\n"]}],"source":["import pandas as pd\n","import spacy\n","\n","# Load the spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Define a function to analyze the text\n","def analyze_text_spacy(text):\n","    doc = nlp(text)\n","    \n","    # Sentence count\n","    sentence_count = len(list(doc.sents))\n","    \n","    # Word count (excluding punctuations)\n","    word_count = len([token for token in doc if not token.is_punct])\n","    \n","    # Vocabulary size (unique words, excluding punctuations)\n","    vocabulary_size = len(set([token.text.lower() for token in doc if not token.is_punct]))\n","    \n","    # Average sentence length\n","    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n","    \n","    return sentence_count, word_count, vocabulary_size, avg_sentence_length\n","\n","# Apply the analysis function to each row in the 'text' column\n","df_dataset[['sentence_count', 'word_count', 'vocabulary_size', 'avg_sentence_length']] = df_dataset['text'].apply(\n","    lambda x: pd.Series(analyze_text_spacy(x))\n",")\n","\n","# To find the total number of tokens, you can sum up the word counts\n","total_tokens = df_dataset['word_count'].sum()\n","\n","# total sentence count\n","total_sentence_count = df_dataset['sentence_count'].sum()\n","\n","#total word_count\n","total_word_count = df_dataset['word_count'].sum()\n","\n","#total vocabulary\n","total_vocabulary_count = df_dataset['vocabulary_size'].sum()\n","\n","#avg sentence_length\n","avg_sentence_length = df_dataset['avg_sentence_length'].mean()\n","\n","# Display the overall analysis results and the updated DataFrame\n","print(f\"Total number of tokens: {total_tokens}\")\n","print(f\"Total number of sentences: {total_sentence_count}\")\n","print(f\"Total number of words: {total_word_count}\")\n","print(f\"Vocabulary Size: {total_vocabulary_count}\")\n","print(f\"Average sentence length: {avg_sentence_length}\")\n","\n","# print(df_dataset.head())  # Adjust according to how you wish t"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting textblob\n","  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: nltk>=3.1 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from textblob) (3.8.1)\n","Requirement already satisfied: click in /Users/sachin/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (8.0.4)\n","Requirement already satisfied: joblib in /Users/sachin/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /Users/sachin/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (2022.7.9)\n","Requirement already satisfied: tqdm in /Users/sachin/anaconda3/lib/python3.11/site-packages (from nltk>=3.1->textblob) (4.65.0)\n","Installing collected packages: textblob\n","Successfully installed textblob-0.17.1\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/sachin/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["!pip install textblob\n","\n","import nltk\n","nltk.download('punkt')\n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of tokens: 55107\n","Total number of sentences: 7787\n","Total number of words: 55107\n","Vocabulary Size: 53904\n","Average sentence length: 7.5706718468255785\n"]}],"source":["from textblob import TextBlob\n","\n","# Function to analyze text\n","def analyze_text_with_textblob(text):\n","    blob = TextBlob(text)\n","    \n","    # Sentence count\n","    sentence_count = len(blob.sentences)\n","    \n","    # Word count (including punctuation)\n","    word_count = len(blob.words)\n","    \n","    # Vocabulary size (unique words, including punctuation)\n","    vocabulary = set(blob.words)\n","    vocabulary_size = len(vocabulary)\n","    \n","    # Average sentence length in terms of words\n","    avg_sentence_length = word_count / sentence_count if sentence_count > 0 else 0\n","    \n","    return sentence_count, word_count, vocabulary_size, avg_sentence_length\n","\n","# Assuming 'df' is your DataFrame and it has a column 'text' with the text you want to analyze\n","results = df_dataset['text'].apply(lambda x: analyze_text_with_textblob(x))\n","\n","# Unpack the results into separate columns\n","df_dataset['sentence_count'], df_dataset['word_count'], df_dataset['vocabulary_size'], df_dataset['avg_sentence_length'] = zip(*results)\n","\n","# To find the total number of tokens, you can sum up the word counts\n","total_tokens = df_dataset['word_count'].sum()\n","\n","# total sentence count\n","total_sentence_count = df_dataset['sentence_count'].sum()\n","\n","#total word_count\n","total_word_count = df_dataset['word_count'].sum()\n","\n","#total vocabulary\n","total_vocabulary_count = df_dataset['vocabulary_size'].sum()\n","\n","#avg sentence_length\n","avg_sentence_length = df_dataset['avg_sentence_length'].mean()\n","\n","# Display the overall analysis results and the updated DataFrame\n","print(f\"Total number of tokens: {total_tokens}\")\n","print(f\"Total number of sentences: {total_sentence_count}\")\n","print(f\"Total number of words: {total_word_count}\")\n","print(f\"Vocabulary Size: {total_vocabulary_count}\")\n","print(f\"Average sentence length: {avg_sentence_length}\")\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOHJZdhYcxqIh3Ceys6muaK","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
